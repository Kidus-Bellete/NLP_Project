{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMB+sIY+p/1biLV0QeU3m3y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kidus-Bellete/NLP_Project/blob/main/Copy_of_nlp_assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Natural Language Processing Assignment I**\n",
        "#     Classification of Texts using Wikipedia"
      ],
      "metadata": {
        "id": "tM9JIjLn4pj_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mWM6JBM3EWN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "MCvJPKpx3uYm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73052ddc-ac9a-4bbf-f007-b34e537ae6eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titles = [\"Medicine\", \"Hospital\", \"Surgery\", \"Health\", \"Heart\", \"Vaccine\",\"endurance\",\"brain\",\"stomach\",\"Therapy\",\n",
        "          \"Pharmacy\", \"Immunology\", \"Pathology\", \"Treatment\", \"Diabetes\", \"Disease\",\n",
        "          \"Therapy\", \"Dentistry\", \"Kidney\", \"Blood\", \"Blood pressure\", \"Virus\",\n",
        "          \"Art\", \"Language\", \"Literature\", \"Philosophy\",\n",
        "          \"Empire\", \"Space\", \"Environment\", \"Color\", \"Mountain\",\"rule of law\",\"justice\",\n",
        "          \"Forest\", \"Cooking\", \"Theology\", \"Fashion\",\"animal\",\"love\",\"tree\",\n",
        "          \"History\", \"Geography\", \"Archaeology\", \"government\", \"Astronomy\"]\n",
        "\n",
        "# Create a data frame\n",
        "df = pd.DataFrame(columns=[\"Text\", \"Label\"])\n",
        "dataArr = []\n",
        "\n",
        "def fetch_data(title):\n",
        "    url = 'https://en.wikipedia.org/w/api.php'\n",
        "    params = {\n",
        "        'action': 'query',\n",
        "        'format': 'json',\n",
        "        'titles': title,\n",
        "        'prop': 'extracts',\n",
        "        'exintro': True,\n",
        "        'explaintext': True,\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    data = response.json()\n",
        "    return data\n",
        "\n",
        "# Fetch data and preprocess text\n",
        "for topic in titles:\n",
        "    data = fetch_data(topic)\n",
        "    topic_data = next(iter(data['query']['pages'].values()))\n",
        "    dataArr.append(topic_data['extract'][:500])\n",
        "\n",
        "data = {\"Text\": dataArr, \"Label\": [\"Medical\"] * 22 + [\"Non-Medical\"] * 23}\n",
        "try:\n",
        "    df_added = pd.DataFrame(data)\n",
        "    df = pd.concat([df, df_added], ignore_index=True)\n",
        "    df.to_csv('nlp_dataset.csv', index=False)\n",
        "\n",
        "    # Tokenization, lemmatization, and stemming function\n",
        "    def preprocess_text(text):\n",
        "        # Tokenization\n",
        "        tokens = word_tokenize(text.lower())  # Convert to lowercase for consistency\n",
        "\n",
        "        # Remove stopwords\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
        "\n",
        "        # Lemmatization\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "        # Stemming\n",
        "        stemmer = PorterStemmer()\n",
        "        tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    # Apply preprocessing to the entire dataset\n",
        "    df['Text'] = df['Text'].apply(preprocess_text)\n",
        "\n",
        "except ValueError as e:\n",
        "    print(f\"ValueError: {e}\")\n",
        "\n",
        "#print(df)\n",
        "\n",
        "# Display the information about the dataset\n",
        "print(\"Dataset Information:\")\n",
        "print(df.info())\n",
        "\n",
        "# Display the first few rows of nlp_project dataset\n",
        "print(\"\\nFirst 26 rows of the dataset:\")\n",
        "print(df.head(26)) # the first 26 columns\n",
        "\n",
        "# Display the distribution of labels\n",
        "print(\"\\nLabel Distribution:\")\n",
        "print(df['Label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFqkjhMM3Ppf",
        "outputId": "b1b2e566-6fa2-4093-9c27-e9bb1e2b5afe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 45 entries, 0 to 44\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Text    45 non-null     object\n",
            " 1   Label   45 non-null     object\n",
            "dtypes: object(2)\n",
            "memory usage: 848.0+ bytes\n",
            "None\n",
            "\n",
            "First 26 rows of the dataset:\n",
            "                                                 Text        Label\n",
            "0   medicin scienc practic care patient manag diag...      Medical\n",
            "1   hospit healthcar institut provid patient treat...      Medical\n",
            "2   surgeri medic specialti us manual instrument t...      Medical\n",
            "3   common usag medicin health accord world health...      Medical\n",
            "4   heart muscular organ anim organ pump blood blo...      Medical\n",
            "5   vaccin biolog prepar provid activ acquir immun...      Medical\n",
            "6   endur also relat suffer forbear resili constit...      Medical\n",
            "7   brain encephalon organ serf center nervou syst...      Medical\n",
            "8   stomach muscular hollow organ gastrointestin t...      Medical\n",
            "9   therapi medic treatment attempt remedi health ...      Medical\n",
            "10  pharmaci scienc practic discov produc prepar d...      Medical\n",
            "11  immunolog branch biolog medicin cover studi im...      Medical\n",
            "12  patholog studi diseas injuri word patholog als...      Medical\n",
            "13  treatment may refer treatment song 2012 song l...      Medical\n",
            "14  diabet mellitu often known simpli diabet group...      Medical\n",
            "15  diseas particular abnorm condit advers affect ...      Medical\n",
            "16  therapi medic treatment attempt remedi health ...      Medical\n",
            "17  dentistri also known dental medicin oral medic...      Medical\n",
            "18  human kidney two organ multilobar multipapilla...      Medical\n",
            "19  blood bodi fluid circulatori system human vert...      Medical\n",
            "20  blood pressur bp pressur circul blood wall blo...      Medical\n",
            "21  viru submicroscop infecti agent replic insid l...      Medical\n",
            "22  art divers rang human activ result product inv...  Non-Medical\n",
            "23  languag structur system commun consist grammar...  Non-Medical\n",
            "24  literatur collect written work also use narrow...  Non-Medical\n",
            "25  philosophi love wisdom ancient greek systemat ...  Non-Medical\n",
            "\n",
            "Label Distribution:\n",
            "Non-Medical    23\n",
            "Medical        22\n",
            "Name: Label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# data validation and read data if its valid\n",
        "try:\n",
        "    df = pd.read_csv('nlp_dataset.csv')\n",
        "    assert 'Text' in df.columns and 'Label' in df.columns, \"Dataset should have 'Text' and 'Label' columns.\"\n",
        "    assert not df[['Text', 'Label']].isnull().values.any(), \"Dataset should not contain missing values.\"\n",
        "except Exception as e:\n",
        "    print(f\"Error in data validation: {e}\")\n",
        "    # Handle the error or exit the program\n",
        "\n",
        "#print the orginal distribution\n",
        "print(\"Original data distribution..\")\n",
        "print(df['Label'].value_counts())\n",
        "\n",
        "# training and testing split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['Label'], test_size=0.3, random_state=42)\n",
        "\n",
        "# Fill missing values\n",
        "X_train = X_train.fillna('')\n",
        "X_test = X_test.fillna('')\n",
        "\n",
        "# feature extraction\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Resampling using SMOTE\n",
        "sampler = SMOTE(sampling_strategy='auto', k_neighbors=5)\n",
        "X_train_resampled, y_train_resampled = sampler.fit_resample(X_train_tfidf, y_train)\n",
        "\n",
        "# Model selection and training\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# model evaluation\n",
        "predictions = model.predict(X_test_tfidf)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"\\nAccuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
        "\n",
        "# Overfitting Check (using validation set)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_resampled, y_train_resampled, test_size=0.2, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "val_predictions = model.predict(X_val)\n",
        "val_accuracy = accuracy_score(y_val, val_predictions)\n",
        "print(f\"\\nValidation Accuracy: {val_accuracy}\")\n",
        "\n",
        "# making predictions\n",
        "# data to predict\n",
        "new_data = [\"headache is a type of disease which attack to humans\",\n",
        "            \"Ethiopia is the oldest city with many historical heritages.\",\n",
        "            \"He has a chance to recover from his disease\",\n",
        "            \"I love to see Italian historical places.\",\n",
        "            \"Have even been in such kind of discomfort situation? \",\n",
        "            \"How can I feel better and to improve my mood?\",\n",
        "            \"do you have health insurance from the company called Wei Italy?\",\n",
        "            \"does eating food changes my feeling?\"\n",
        "            #add any sentences to check its prediction value\n",
        "            ]\n",
        "\n",
        "new_data_tfidf = tfidf_vectorizer.transform(new_data)\n",
        "new_predictions = model.predict(new_data_tfidf)\n",
        "\n",
        "print(\"\\nPredictions on new data:\\n\")\n",
        "\n",
        "for text, prediction in zip(new_data, new_predictions):\n",
        "    print(f\"{text} - Predicted: {prediction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu0HNaBM4OyW",
        "outputId": "6b68434f-3f46-46c0-9b04-980dbd62d3c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data distribution..\n",
            "Non-Medical    23\n",
            "Medical        22\n",
            "Name: Label, dtype: int64\n",
            "\n",
            "Accuracy: 0.9285714285714286\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     Medical       1.00      0.83      0.91         6\n",
            " Non-Medical       0.89      1.00      0.94         8\n",
            "\n",
            "    accuracy                           0.93        14\n",
            "   macro avg       0.94      0.92      0.93        14\n",
            "weighted avg       0.94      0.93      0.93        14\n",
            "\n",
            "\n",
            "Validation Accuracy: 1.0\n",
            "\n",
            "Predictions on new data:\n",
            "\n",
            "headache is a type of disease which attack to humans - Predicted: Medical\n",
            "Ethiopia is the oldest city with many historical heritages. - Predicted: Non-Medical\n",
            "He has a chance to recover from his disease - Predicted: Medical\n",
            "I love to see Italian historical places. - Predicted: Non-Medical\n",
            "Have even been in such kind of discomfort situation?  - Predicted: Non-Medical\n",
            "How can I feel better and to improve my mood? - Predicted: Medical\n",
            "do you have health insurance from the company called Wei Italy? - Predicted: Medical\n",
            "does eating food changes my feeling? - Predicted: Non-Medical\n"
          ]
        }
      ]
    }
  ]
}